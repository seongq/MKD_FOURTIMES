Epoch: 0:
Start training ...
Traceback (most recent call last):
  File "/workspace/emotion_KD250628/MKD/CREMA_D_MKD/MKD/main.py", line 506, in <module>
    main()
  File "/workspace/emotion_KD250628/MKD/CREMA_D_MKD/MKD/main.py", line 408, in main
    batch_loss, batch_loss_a, batch_loss_v, global_step = train_epoch(args, epoch, model, device,
  File "/workspace/emotion_KD250628/MKD/CREMA_D_MKD/MKD/main.py", line 123, in train_epoch
    loss.backward()
  File "/opt/conda/lib/python3.10/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 23.69 GiB of which 4.19 MiB is free. Process 4157444 has 15.71 GiB memory in use. Process 446415 has 1.77 GiB memory in use. Process 446761 has 1.75 GiB memory in use. Process 447399 has 2.51 GiB memory in use. Process 448126 has 1.91 GiB memory in use. Of the allocated memory 1.44 GiB is allocated by PyTorch, and 69.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
